// netlify/edge-functions/rag_chatbot.js

// ============================================================================
// CONFIGURATION - ModÃ¨les et paramÃ¨tres par dÃ©faut
// ============================================================================

import {
  loadInstanceConfig,
  getConfig,
  getSupabase,
} from "../../common/config/instanceConfig.edge.js";

import createClient from "@supabase/supabase-js";

// TODO: should load OpenAI from bundle, not from esm.sh
// import OpenAI from "../../common/lib/openai.js";
import OpenAI from "https://esm.sh/openai@4";

const PROVIDER_META_PREFIX = "__PROVIDER_INFO__";
import { providerMetrics } from "./lib/utils/provider-metrics.js";
const PROVIDERS_STATUS_PREFIX = "__PROVIDERS_STATUS__";
const TOOL_TRACE_PREFIX = "__TOOL_TRACE__";

const MODEL_MODES = {
  mistral: {
    fast: "mistral-small-latest",
    strong: "mistral-large-latest",
    reasoning: "magistral-medium-latest",
  },

  anthropic: {
    main: "claude-sonnet-4-5-20250929",
    cheap: "claude-3-haiku-20240307",
  },

  openai: {
    main: "gpt-4.1",
    reasoning: "gpt-5.1",
    cheap: "gpt-4.1-nano",
  },

  google: {
    // Le modÃ¨le le plus intelligent (Gemini 3)
    main: "gemini-3-pro-preview",
    // Le modÃ¨le rapide et stable (Gemini 2.5 Flash)
    fast: "gemini-2.5-flash",
    // ModÃ¨le de raisonnement avancÃ© (Thinking)
    reasoning: "gemini-2.0-flash-thinking-exp",
    // Pas cher
    cheap: "gemini-2.5-flash-lite",
  },

  huggingface: {
    // Chat gÃ©nÃ©raliste (non limitÃ© au reasoning)
    main: "deepseek-ai/DeepSeek-V3",
    // Version plus lÃ©gÃ¨re (distill, toujours capable de reasoning mais moins coÃ»teuse)
    small: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    // Gros modÃ¨le reasoning quand tu veux lâ€™artillerie lourde
    reasoning: "deepseek-ai/DeepSeek-R1",
  },
};

const DEFAULT_MODEL_MODE = {
  mistral: "fast",
  anthropic: "main",
  openai: "reasoning", // ChangÃ© Ã  reasoning pour gpt-5.1
  huggingface: "main",
  google: "main",
};

const MODEL_MODE_DIRECTIVE_REGEX = /model_mode\s*=\s*([^\s;]+)/i;

const resolveModelForProvider = (provider, overrideMode) => {
  const providerModes = MODEL_MODES[provider];
  if (!providerModes) {
    console.warn(`[resolveModel] No modes defined for provider: ${provider}`);
    return undefined;
  }

  const candidateMode =
    overrideMode && providerModes[overrideMode]
      ? overrideMode
      : DEFAULT_MODEL_MODE[provider] || Object.keys(providerModes)[0];

  const resolved = providerModes[candidateMode];
  return resolved;
};

// ============================================================================
// OUTILS (TOOLS) - DÃ©finition centralisÃ©e
// ============================================================================

const TOOLS = {
  web_search: {
    name: "web_search",
    description:
      "Recherche des informations actualisÃ©es sur Internet. Utilise cet outil pour des questions sur des actualitÃ©s, horaires, ou donnÃ©es externes (ex: 'horaires mairie corte 2025').",
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: "RequÃªte de recherche courte et prÃ©cise (3-8 mots).",
          minLength: 3,
          maxLength: 50,
        },
      },
      required: ["query"],
    },
  },
  vector_search: {
    name: "vector_search",
    description:
      "Recherche dans la base de connaissances locale pour des questions sur l'histoire locale, Ã©vÃ©nements passÃ©s, conseils municipaux, etc.",
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: "Question ou requÃªte de recherche en franÃ§ais.",
        },
        source_type: {
          type: "string",
          description:
            "Optional filter to only search chunks from a specific source_type (e.g., 'wiki_page').",
        },
        domain: {
          type: "string",
          description: "Optional filter for domain field (e.g., 'wiki', 'history').",
        },
        limit: {
          type: "integer",
          description: "Maximum number of results to return (defaults to 5).",
        },
      },
      required: ["query"],
    },
  },
  wiki_search: {
    name: "wiki_search",
    description:
      "Search within the wiki pages indexed in the knowledge_chunks table (source_type = 'wiki_page').",
    parameters: {
      type: "object",
      properties: {
        query: { type: "string", description: "Question or search query" },
        limit: { type: "integer", description: "Max results to return" },
      },
      required: ["query"],
    },
  },
  sql_query: {
    name: "sql_query",
    description:
      "Execute a read-only SQL query against the database for advanced data access. Only SELECT queries are allowed. The model should target the condensed schema below and return only requested columns. Avoid UPDATE/INSERT/DELETE. Responses are JSON by default unless you request markdown explicitly.",
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: "The SQL SELECT query to execute. Must be read-only (SELECT only).",
        },
        limit: {
          type: "integer",
          description: "Maximum number of rows to return (default 100).",
        },
        format: {
          type: "string",
          enum: ["json", "markdown"],
          description:
            "Output format. Defaults to 'json'. Set to 'markdown' only when a tabular rendition is explicitly required.",
        },
      },
      required: ["query"],
    },
  },
  create_post: {
    name: "create_post",
    description:
      "Publie un nouveau message, une annonce ou une pensÃ©e. Ne PAS utiliser pour des tÃ¢ches ou des propositions.",
    parameters: {
      type: "object",
      properties: {
        content: { type: "string", description: "Le contenu du message (Markdown supportÃ©)." },
        title: { type: "string", description: "Titre optionnel du message." },
        group_id: { type: "string", description: "ID du groupe oÃ¹ publier (optionnel)." },
        tags: {
          type: "array",
          items: { type: "string" },
          description: "Liste de tags (ex: ['urgent', 'event']).",
        },
      },
      required: ["content"],
    },
  },
  update_post: {
    name: "update_post",
    description: "Modifie un message existant.",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID du message Ã  modifier." },
        content: { type: "string", description: "Nouveau contenu." },
        title: { type: "string", description: "Nouveau titre." },
      },
      required: ["id"],
    },
  },
  list_posts: {
    name: "list_posts",
    description: "Liste les messages rÃ©cents, filtrables par groupe ou type.",
    parameters: {
      type: "object",
      properties: {
        group_id: { type: "string", description: "Filtrer par groupe." },
        limit: { type: "integer", description: "Nombre max de rÃ©sultats (dÃ©faut 10)." },
        query: { type: "string", description: "Recherche textuelle dans le contenu." },
      },
    },
  },
  create_task: {
    name: "create_task",
    description: "CrÃ©e une nouvelle tÃ¢che dans un projet ou un groupe.",
    parameters: {
      type: "object",
      properties: {
        title: { type: "string", description: "Titre de la tÃ¢che." },
        description: { type: "string", description: "Description dÃ©taillÃ©e." },
        project_id: {
          type: "string",
          description: "ID du projet (groupe type task_project) ou groupe parent.",
        },
        status: {
          type: "string",
          enum: ["todo", "in_progress", "done", "blocked"],
          description: "Statut initial.",
        },
        priority: { type: "string", enum: ["low", "medium", "high"], description: "PrioritÃ©." },
        assignee_id: { type: "string", description: "ID de l'utilisateur assignÃ©." },
      },
      required: ["title"],
    },
  },
  update_task: {
    name: "update_task",
    description: "Met Ã  jour une tÃ¢che (statut, assignation, dÃ©tails).",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID de la tÃ¢che." },
        status: { type: "string", enum: ["todo", "in_progress", "done", "blocked"] },
        title: { type: "string" },
        description: { type: "string" },
        priority: { type: "string" },
      },
      required: ["id"],
    },
  },
  list_tasks: {
    name: "list_tasks",
    description: "Liste les tÃ¢ches, filtrables par projet, statut ou assignation.",
    parameters: {
      type: "object",
      properties: {
        project_id: { type: "string", description: "ID du projet." },
        status: { type: "string", description: "Filtrer par statut." },
        assignee_id: { type: "string", description: "Filtrer par assignÃ© (me = moi)." },
        limit: { type: "integer", description: "Max rÃ©sultats." },
      },
    },
  },
  create_mission: {
    name: "create_mission",
    description: "CrÃ©e une nouvelle mission (groupe d'action).",
    parameters: {
      type: "object",
      properties: {
        name: { type: "string", description: "Nom de la mission." },
        description: { type: "string", description: "Objectif de la mission." },
        location: { type: "string", description: "Lieu (optionnel)." },
      },
      required: ["name"],
    },
  },
  update_mission: {
    name: "update_mission",
    description: "Met Ã  jour une mission.",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID de la mission." },
        name: { type: "string" },
        description: { type: "string" },
        location: { type: "string" },
        status: { type: "string", enum: ["active", "completed", "archived"] },
      },
      required: ["id"],
    },
  },
  list_missions: {
    name: "list_missions",
    description: "Liste les missions disponibles.",
    parameters: {
      type: "object",
      properties: {
        query: { type: "string", description: "Recherche par nom." },
        limit: { type: "integer", description: "Max rÃ©sultats." },
      },
    },
  },
  join_group: {
    name: "join_group",
    description: "Rejoint un groupe ou une mission.",
    parameters: {
      type: "object",
      properties: {
        group_id: { type: "string", description: "ID du groupe Ã  rejoindre." },
      },
      required: ["group_id"],
    },
  },
  leave_group: {
    name: "leave_group",
    description: "Quitte un groupe ou une mission.",
    parameters: {
      type: "object",
      properties: {
        group_id: { type: "string", description: "ID du groupe Ã  quitter." },
      },
      required: ["group_id"],
    },
  },
  list_my_groups: {
    name: "list_my_groups",
    description: "Liste les groupes dont je suis membre.",
    parameters: {
      type: "object",
      properties: {},
    },
  },
  create_proposition: {
    name: "create_proposition",
    description: "CrÃ©e une proposition pour le vote.",
    parameters: {
      type: "object",
      properties: {
        title: { type: "string", description: "Titre de la proposition." },
        description: { type: "string", description: "Description dÃ©taillÃ©e." },
        tags: {
          type: "array",
          items: { type: "string" },
          description: "Tags associÃ©s (ex: ['urbanisme', 'budget']).",
        },
      },
      required: ["title"],
    },
  },
  update_proposition: {
    name: "update_proposition",
    description: "Met Ã  jour une proposition.",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID de la proposition." },
        status: { type: "string", enum: ["active", "closed", "draft"] },
        title: { type: "string" },
      },
      required: ["id"],
    },
  },
  list_propositions: {
    name: "list_propositions",
    description: "Liste les propositions actives.",
    parameters: {
      type: "object",
      properties: {
        status: { type: "string", description: "Filtrer par statut (dÃ©faut: active)." },
        tag: { type: "string", description: "Filtrer par tag." },
        limit: { type: "integer", description: "Max rÃ©sultats." },
      },
    },
  },
  vote_proposition: {
    name: "vote_proposition",
    description: "Vote pour ou contre une proposition.",
    parameters: {
      type: "object",
      properties: {
        proposition_id: { type: "string", description: "ID de la proposition." },
        value: {
          type: "integer",
          enum: [1, -1, 0],
          description: "1 (Pour), -1 (Contre), 0 (Neutre/Retrait).",
        },
      },
      required: ["proposition_id", "value"],
    },
  },
  create_wiki_page: {
    name: "create_wiki_page",
    description: "CrÃ©e une nouvelle page Wiki.",
    parameters: {
      type: "object",
      properties: {
        title: { type: "string", description: "Titre de la page." },
        content: { type: "string", description: "Contenu (Markdown)." },
        summary: { type: "string", description: "RÃ©sumÃ© court." },
      },
      required: ["title", "content"],
    },
  },
  update_wiki_page: {
    name: "update_wiki_page",
    description: "Met Ã  jour une page Wiki.",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID de la page." },
        content: { type: "string" },
        summary: { type: "string" },
      },
      required: ["id"],
    },
  },
  get_wiki_page: {
    name: "get_wiki_page",
    description: "RÃ©cupÃ¨re le contenu d'une page Wiki.",
    parameters: {
      type: "object",
      properties: {
        id: { type: "string", description: "ID de la page (optionnel si title fourni)." },
        title: { type: "string", description: "Titre exact (optionnel si id fourni)." },
      },
    },
  },
  add_reaction: {
    name: "add_reaction",
    description: "Ajoute une rÃ©action (emoji) Ã  un post.",
    parameters: {
      type: "object",
      properties: {
        post_id: { type: "string", description: "ID du post." },
        emoji: { type: "string", description: "Emoji (ex: 'ðŸ‘', 'â¤ï¸')." },
      },
      required: ["post_id", "emoji"],
    },
  },
  create_comment: {
    name: "create_comment",
    description: "Ajoute un commentaire Ã  un post.",
    parameters: {
      type: "object",
      properties: {
        post_id: { type: "string", description: "ID du post." },
        content: { type: "string", description: "Contenu du commentaire." },
      },
      required: ["post_id", "content"],
    },
  },
  get_schema_info: {
    name: "get_schema_info",
    description: "Retourne des informations sur la structure de la base de donnÃ©es.",
    parameters: {
      type: "object",
      properties: {
        table: { type: "string", description: "Nom de la table (optionnel)." },
      },
    },
  },
  get_user_context: {
    name: "get_user_context",
    description: "Retourne les informations sur l'utilisateur actuel et le contexte de navigation.",
    parameters: {
      type: "object",
      properties: {},
    },
  },
  list_capabilities: {
    name: "list_capabilities",
    description: "Liste tous les outils disponibles pour l'agent.",
    parameters: {
      type: "object",
      properties: {},
    },
  },
  // Ajoute d'autres outils ici (ex: search_local_db, weather, etc.)
};

// ============================================================================
// GESTIONNAIRES D'OUTILS - Fonctions d'exÃ©cution
// ============================================================================

const TOOL_HANDLERS = {
  web_search({ query }) {
    return performWebSearch(query);
  },
  async vector_search({ query, source_type, domain, limit = 5 }, { supabase, openai }) {
    console.log(`[VectorSearch] âžœ query=${previewForLog(query)}`);
    if (!supabase || !openai) {
      return `Recherche vectorielle non configurÃ©e.`;
    }
    try {
      // Embed the query
      const embeddingResponse = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: query,
      });
      const queryEmbedding = embeddingResponse.data[0].embedding;

      // Fetch chunks (limit to 1000 for performance)
      let qb = supabase.from("knowledge_chunks").select("id,text,embedding,metadata");
      if (source_type) qb = qb.eq("source_type", source_type);
      if (domain) qb = qb.eq("domain", domain);
      const { data, error } = await qb.limit(1000);

      if (error) {
        console.error(`[VectorSearch] âŒ Supabase error:`, error);
        return `Erreur de recherche: ${error.message}`;
      }

      if (!data || data.length === 0) {
        return "Aucun rÃ©sultat trouvÃ© dans la base de connaissances locale.";
      }

      // Parse embeddings and compute cosine similarity
      const scored = data.map((chunk) => {
        let emb = chunk.embedding;
        if (typeof emb === "string") {
          try {
            emb = JSON.parse(emb);
          } catch {
            emb = emb.split(",").map(Number);
          }
        }
        const similarity = cosineSimilarity(queryEmbedding, emb);
        return { chunk, score: similarity };
      });

      // Sort by similarity descending
      scored.sort((a, b) => b.score - a.score);

      // Take top limit
      const topResults = scored.slice(0, limit);

      let result = `ðŸ“š RÃ©sultats de la recherche locale pour "${query}":\n\n`;
      topResults.forEach((item, i) => {
        const title = item.chunk.metadata?.title || `RÃ©sultat ${i + 1}`;
        result += `ðŸ“„ **${title}**\n`;
        result += `${item.chunk.text.substring(0, 500)}...\n\n`;
      });

      console.log(`[VectorSearch] âœ… ${topResults.length} rÃ©sultats`);
      return result;
    } catch (error) {
      console.error(`[VectorSearch] âŒ Erreur:`, error);
      return `âš ï¸ Erreur de recherche vectorielle: ${error.message}`;
    }
  },
  async wiki_search({ query, limit = 5 }, { supabase, openai }) {
    // Delegate to vector_search with specific filter
    try {
      return await TOOL_HANDLERS.vector_search(
        { query, source_type: "wiki_page", limit },
        { supabase, openai }
      );
    } catch (err) {
      console.error(`[WikiSearch] âŒ Error:`, err);
      return `âš ï¸ Erreur de recherche wiki: ${err.message}`;
    }
  },

  async sql_query({ query }, { supabase }) {
    console.log(`[SqlQuery] âžœ query=${previewForLog(query)}`);
    if (!supabase) return "Base de donnÃ©es non configurÃ©e.";
    try {
      // Nettoyage basique
      const cleanQuery = query.trim().replace(/;+$/, "");
      if (
        !cleanQuery.toLowerCase().startsWith("select") &&
        !cleanQuery.toLowerCase().startsWith("with")
      ) {
        return "Erreur: Seules les requÃªtes SELECT (lecture seule) sont autorisÃ©es.";
      }

      // Tentative via RPC exec_sql (standard dans nos instances)
      const { data, error } = await supabase.rpc("exec_sql", { sql_query: cleanQuery });
      if (error) {
        console.warn(
          `[SqlQuery] âš ï¸ RPC exec_sql failed, trying direct query if admin: ${error.message}`
        );
        // Fallback: si c'est un client admin, on pourrait tenter autre chose,
        // mais l'RPC est la mÃ©thode privilÃ©giÃ©e pour les Edge Functions.
        return `Erreur SQL: ${error.message}`;
      }

      const rows = Array.isArray(data) ? data : data?.rows || [];
      console.log(`[SqlQuery] âœ… ${rows.length} lignes retournÃ©es`);
      return JSON.stringify(rows);
    } catch (error) {
      console.error(`[SqlQuery] âŒ Erreur:`, error);
      return `âš ï¸ Erreur d'exÃ©cution SQL: ${error.message}`;
    }
  },

  get_user_context(_, { user, context }) {
    console.log(`[UserContext] âžœ user=${user?.id || "anonymous"}`);
    return JSON.stringify({
      user: user
        ? {
            id: user.id,
            email: user.email,
            metadata: user.user_metadata,
            role: user.role,
          }
        : null,
      navigation_context: context || {},
      timestamp: new Date().toISOString(),
    });
  },

  list_capabilities() {
    console.log(`[Capabilities] âžœ listing ${Object.keys(TOOLS).length} tools`);
    return JSON.stringify(
      Object.values(TOOLS).map((t) => ({
        name: t.name,
        description: t.description,
      }))
    );
  },

  // Ajoute d'autres handlers ici
};

function formatMarkdownCell(value) {
  if (value == null) return "";
  if (typeof value === "object") {
    try {
      return `\`${JSON.stringify(value)}\``;
    } catch {
      return String(value);
    }
  }
  return String(value);
}

// UTIL: small preview helper for logs
function previewForLog(value, max = 400) {
  try {
    const s = typeof value === "string" ? value : JSON.stringify(value);
    return s.length > max ? s.slice(0, max) + "..." : s;
  } catch {
    return String(value).slice(0, max) + (String(value).length > max ? "..." : "");
  }
}

// Vector similarity helpers
function dot(a, b) {
  let s = 0;
  for (let i = 0; i < a.length; i++) s += a[i] * b[i];
  return s;
}
function norm(a) {
  return Math.sqrt(dot(a, a));
}
function cosineSimilarity(a, b) {
  return dot(a, b) / (norm(a) * norm(b));
}

// ============================================================================
// BRAVE SEARCH - Outil de recherche web (amÃ©liorÃ©)
// ============================================================================

async function performWebSearch(query) {
  console.log(`[WebSearch] âžœ request query=${previewForLog(query)}`);
  const apiKey = getConfig("BRAVE_SEARCH_API_KEY");
  if (!apiKey) {
    console.warn("[WebSearch] âš ï¸ BRAVE_SEARCH_API_KEY manquant");
    return `Recherche web non configurÃ©e pour: "${query}". RÃ©ponds en t'excusant et en proposant une alternative si possible.`;
  }

  try {
    const url = new URL("https://api.search.brave.com/res/v1/web/search");
    url.searchParams.append("q", query);
    url.searchParams.append("count", "10");
    url.searchParams.append("search_lang", "fr");
    url.searchParams.append("country", "FR");

    console.log(`[WebSearch] ðŸŒ fetch url=${previewForLog(url.toString())}`);
    const response = await fetch(url.toString(), {
      headers: {
        Accept: "application/json",
        "X-Subscription-Token": apiKey,
      },
    });

    console.log(`[WebSearch] â¬… status=${response.status}`);
    if (!response.ok) throw new Error(`Brave API: ${response.status}`);

    const data = await response.json();
    console.log(`[WebSearch] â¬… data preview: ${previewForLog(data)}`);

    let resultText = `ðŸ” RÃ©sultats pour "${query}":\n\n`;

    // RÃ©sultats web classiques
    if (data.web?.results?.length > 0) {
      data.web.results.slice(0, 10).forEach((result, i) => {
        resultText += `ðŸ“„ ${i + 1}. **${result.title}**\n`;
        resultText += `${result.description?.substring(0, 300) || "Pas de description"}...\n`;
        resultText += `ðŸ”— [Source](${result.url})\n\n`;
      });
    } else {
      resultText += "Aucun rÃ©sultat web trouvÃ©.\n\n";
    }

    // RÃ©sultats locaux
    if (data.locations?.results?.length > 0) {
      resultText += `ðŸ“ **Infos locales :**\n`;
      data.locations.results.slice(0, 10).forEach((loc) => {
        resultText += `- **${loc.title}**\n`;
        if (loc.address) resultText += `  ðŸ“ ${loc.address}\n`;
        if (loc.phone) resultText += `  ðŸ“ž ${loc.phone}\n`;
        if (loc.hours) resultText += `  â° ${loc.hours}\n`;
      });
    }

    console.log(`[WebSearch] âœ… ${data.web?.results?.length || 0} rÃ©sultats formatÃ©s`);
    return resultText;
  } catch (error) {
    console.error("[WebSearch] âŒ Erreur:", error.message);
    return `âš ï¸ Erreur de recherche: ${error.message}. Je ne peux pas accÃ©der Ã  Internet pour le moment.`;
  }
}

// ============================================================================
// UTILITAIRES - Fonctions communes
// ============================================================================
const parseToolArguments = (raw) => {
  if (!raw) return {};
  try {
    return typeof raw === "string" ? JSON.parse(raw) : raw;
  } catch {
    return {};
  }
};

const isAsyncIterable = (value) =>
  Boolean(value && typeof value[Symbol.asyncIterator] === "function");

// Update executeToolCalls to accept a fallbackContext for missing arguments
// Update executeToolCalls to support parallel execution and pass user context
async function executeToolCalls(
  toolCalls,
  provider = "mistral",
  fallbackContext = {},
  supabase,
  openai,
  metaCollector = null,
  toolEventEmitter = null,
  debugMode = false,
  user = null,
  context = {}
) {
  console.log(`[${provider}] ðŸ” executeToolCalls parallel called count=${toolCalls.length}`);
  const toolPromises = toolCalls.map(async (call) => {
    try {
      const toolName = call.function?.name || call.name;
      let args = parseToolArguments(call.function?.arguments || call.arguments);
      console.log(`[${provider}] âžœ Tool call: ${toolName} args=${previewForLog(args, 400)}`);

      // Apply fallback logic for web_search: use question if query is missing
      if (toolName === "web_search") {
        if (!args || !args.query) {
          const fallbackQuery = fallbackContext?.web_search?.query || fallbackContext?.defaultQuery;
          if (fallbackQuery && typeof fallbackQuery === "string" && fallbackQuery.trim()) {
            args = { ...args, query: fallbackQuery };
            console.log(`[${provider}] â„¹ï¸ web_search fallback -> query="${fallbackQuery}"`);
          }
        }
      }

      // Validate required parameters based on TOOLS definition
      const toolDef = Object.values(TOOLS).find((t) => t.name === toolName);
      if (toolDef) {
        const required = toolDef.parameters?.required || [];
        let hasAllRequired = true;
        for (const r of required) {
          if (
            !args ||
            args[r] === undefined ||
            args[r] === null ||
            (typeof args[r] === "string" && args[r].trim() === "")
          ) {
            hasAllRequired = false;
            break;
          }
        }
        if (!hasAllRequired) {
          console.warn(
            `[${provider}] âš ï¸ ParamÃ¨tres manquants pour ${toolName} (call id=${call.id}). IgnorÃ©.`
          );
          return {
            role: "tool",
            tool_call_id: call.id,
            name: toolName,
            content: `Erreur: ParamÃ¨tres requis manquants pour ${toolName}.`,
          };
        }
      }

      const handler = TOOL_HANDLERS[toolName];
      if (!handler) {
        console.warn(`[${provider}] Outil non gÃ©rÃ©: ${toolName}`);
        return {
          role: "tool",
          tool_call_id: call.id,
          name: toolName,
          content: `Erreur: Outil "${toolName}" non supportÃ©.`,
        };
      }

      if (toolName === "sql_query" && debugMode) {
        const rawQuery = typeof args?.query === "string" ? args.query.trim() : "";
        if (rawQuery) {
          const preview = previewForLog(rawQuery, 800);
          const debugMessage = `ðŸ’¡ SQL (debug) requÃªte exÃ©cutÃ©e :\n${preview}`;
          toolEventEmitter?.({
            phase: "notice",
            provider,
            tool: toolName,
            callId: call.id,
            message: debugMessage,
            timestamp: Date.now(),
            debugSql: {
              query: rawQuery,
              preview,
            },
          });
        }
      }

      console.log(`[${provider}] ðŸ›  ExÃ©cution de ${toolName} avec:`, args);
      toolEventEmitter?.({
        phase: "start",
        provider,
        tool: toolName,
        callId: call.id,
        timestamp: Date.now(),
        argumentsPreview: previewForLog(args, 200),
      });
      const t0 = Date.now();
      const output = await handler(args, { supabase, openai, debugMode, user, context });
      const t1 = Date.now();
      console.log(
        `[${provider}] â¬… Tool result for ${toolName} preview: ${previewForLog(output, 400)}`
      );

      toolEventEmitter?.({
        phase: "finish",
        provider,
        tool: toolName,
        callId: call.id,
        durationMs: t1 - t0,
        resultPreview: previewForLog(output, 200),
        timestamp: Date.now(),
      });

      if (toolName === "sql_query") {
        toolEventEmitter?.({
          phase: "notice",
          provider,
          tool: toolName,
          callId: call.id,
          message: "ðŸ› ï¸ L'outil SQL a terminÃ©, reprise de la rÃ©ponseâ€¦",
          timestamp: Date.now(),
        });
      }

      if (metaCollector) {
        metaCollector.tool_trace = metaCollector.tool_trace || [];
        metaCollector.tool_trace.push({
          id: call.id,
          name: toolName,
          duration_ms: t1 - t0,
          result_preview: previewForLog(output, 400),
        });
      }

      return {
        role: "tool",
        tool_call_id: call.id,
        name: toolName,
        content: output,
      };
    } catch (error) {
      console.error(`[${provider}] âŒ Erreur outil:`, error);
      toolEventEmitter?.({
        phase: "error",
        provider,
        tool: call.function?.name || call.name,
        callId: call.id,
        error: error?.message || String(error),
        timestamp: Date.now(),
      });
      return {
        role: "tool",
        tool_call_id: call.id,
        name: call.function?.name || call.name,
        content: `âš ï¸ Erreur: ${error.message}`,
      };
    }
  });

  return await Promise.all(toolPromises);
}

// ============================================================================
// APPels API - Gestion unifiÃ©e des LLM (Mistral, Anthropic, OpenAI)
// ============================================================================
const PROVIDER_CONFIGS = {
  mistral: {
    apiUrl: "https://api.mistral.ai/v1/chat/completions",
    defaultModel: "mistral-large-latest",
    toolFormat: "openai", // Mistral utilise le mÃªme format qu'OpenAI
  },
  anthropic: {
    apiUrl: "https://api.anthropic.com/v1/messages",
    defaultModel: "claude-3-opus-20240229",
    toolFormat: "anthropic", // Format spÃ©cifique
  },
  openai: {
    apiUrl: "https://api.openai.com/v1/chat/completions",
    defaultModel: "gpt-4o-mini",
    toolFormat: "openai", // âœ… Identique Ã  Mistral (SSE)
  },
  huggingface: {
    apiUrl: (_model) => `https://router.huggingface.co/v1/chat/completions`,
    defaultModel: "mistralai/Mixtral-8x22B-Instruct-v0.1",
    toolFormat: null, // Pas de support des outils
  },
  google: {
    // Utilisation de l'endpoint de compatibilitÃ© OpenAI de Google
    apiUrl: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions",
    defaultModel: "gemini-2.5-flash",
    toolFormat: "openai", // Gemini via cet endpoint supporte le format OpenAI
  },
};

function formatToolsForProvider(tools, provider) {
  const config = PROVIDER_CONFIGS[provider];
  if (config.toolFormat === "anthropic") {
    return tools.map((tool) => ({
      name: tool.name,
      description: tool.description,
      input_schema: tool.parameters,
    }));
  } else if (config.toolFormat === "openai") {
    return tools.map((tool) => ({
      type: "function",
      function: tool,
    }));
  } else {
    return []; // Pas de support des outils
  }
}

async function callLLMAPI({
  provider,
  model,
  messages,
  _tools = [],
  toolChoice = "auto",
  stream = true,
}) {
  const config = PROVIDER_CONFIGS[provider];
  // GESTION SPÃ‰CIFIQUE POUR LA CLÃ‰ API GEMINI
  let apiKey;
  if (provider === "google") {
    apiKey = getConfig("GEMINI_API_KEY");
  } else {
    apiKey = getConfig(`${provider.toUpperCase()}_API_KEY`);
  }
  if (!apiKey) throw new Error(`ClÃ© API manquante pour ${provider}`);

  const formattedTools = formatToolsForProvider(Object.values(TOOLS), provider);
  const payload = {
    model: model || config.defaultModel,
    messages,
    ...(formattedTools.length ? { tools: formattedTools } : {}),
    ...(toolChoice !== "none" ? { tool_choice: toolChoice } : {}),
    stream: stream && provider !== "huggingface",
    temperature: 0.3,
    top_p: 0.95,
  };

  // Add extended thinking for Anthropic (Claude)
  if (provider === "anthropic") {
    payload.thinking = {
      type: "enabled",
      budget_tokens: 2000, // Adjust based on your needs
    };
  }

  // Debug: request payload summary
  console.log(
    `[LLM] âžœ ${provider} request: model=${payload.model}, messages=${payload.messages?.length || 0}, tools=${formattedTools.length}, stream=${payload.stream}`
  );
  console.log(
    `[LLM] âžœ ${provider} payload preview: ${previewForLog({ model: payload.model, firstMessage: payload.messages?.[0]?.content || "", toolCount: formattedTools.length }, 100)}`
  );

  const apiUrl = typeof config.apiUrl === "function" ? config.apiUrl(model) : config.apiUrl;

  // Headers spÃ©cifiques par provider
  const headers = {
    "Content-Type": "application/json",
  };
  if (provider === "anthropic") {
    headers["x-api-key"] = apiKey;
    headers["anthropic-version"] = "2023-06-01";
  } else {
    headers["Authorization"] = `Bearer ${apiKey}`;
  }

  const response = await fetch(apiUrl, {
    method: "POST",
    headers,
    body: JSON.stringify(payload),
  });

  console.log(`[LLM] â¬… ${provider} response status=${response.status} stream=${stream}`);

  if (!response.ok) {
    const body = await response.text();
    console.error(`[LLM] âŒ ${provider} error body preview: ${previewForLog(body)}`);
    throw new Error(`${provider} API ${response.status}: ${body}`);
  }

  if (!stream || provider === "huggingface") {
    const data = await response.json();
    console.log(`[LLM] â¬… ${provider} non-stream preview: ${previewForLog(data, 1000)}`);
    // For Anthropic we keep legacy handling (thinking blocks, tool_uses normalization).
    // For other providers return the raw JSON so callers can normalize different shapes.
    if (provider === "anthropic") return handleDirectResponse(data, provider);
    return data;
  } else {
    console.log(`[LLM] â¬… ${provider} streaming start`);
    return handleStreamingResponse(response, provider);
  }
}

// Update handleStreamingResponse to yield event objects instead of raw strings
async function* handleStreamingResponse(response, provider) {
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let buffer = "";
  const toolCalls = [];
  let fullContent = "";

  // Buffering for tool call fragments: id -> { name, argsStr }
  const pendingToolArgs = new Map();
  const pushedToolIds = new Set();
  const context = { pendingToolArgs, pushedToolIds, toolCalls, toolFragmentCounter: 0 };

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split("\n");
    buffer = lines.pop() || "";

    for (const line of lines) {
      const trimmed = line.trim();
      if (!trimmed) continue;

      const payload = trimmed.startsWith("data:")
        ? trimmed.slice(trimmed.indexOf(":") + 1).trim()
        : trimmed;
      if (!payload || payload === "[DONE]") continue;

      try {
        // Small preview to help debugging
        const preview = payload.length > 300 ? payload.slice(0, 300) + "..." : payload;
        const data = JSON.parse(payload);
        const delta = provider === "anthropic" ? data.delta : data.choices?.[0]?.delta;
        const hasToolDelta =
          Boolean(delta?.tool_calls?.length) ||
          Boolean(delta?.tool_call) ||
          Boolean(delta?.tool_use?.length);
        const onlyContentDelta =
          Boolean(delta?.content) && !hasToolDelta && !delta?.tool_use?.length;
        const shouldLogPayload = !onlyContentDelta;

        if (shouldLogPayload) {
          console.log(`[${provider}] [SSE] incoming payload preview: ${preview}`);
          console.log(`[${provider}] [SSE] parsed keys: ${Object.keys(data).join(",")}`);
          if (delta) {
            console.log(`[${provider}] [SSE] delta keys: ${Object.keys(delta).join(",")}`);
          }
        }

        if (provider === "anthropic") {
          // Handle thinking blocks (extended thinking feature)
          if (delta?.type === "thinking" && delta?.thinking) {
            // Wrap thinking in <Think> tags for frontend
            const thinkingText = `<Think>${delta.thinking}</Think>`;
            fullContent += thinkingText;
            yield thinkingText;
          }

          // Handle regular text content
          if (delta?.text) {
            fullContent += delta.text;
            yield delta.text;
          }

          // Handle tool calls
          const calls = delta?.tool_use ? delta.tool_use.map(normalizeToolCall) : [];
          if (calls.length) toolCalls.push(...calls);
        } else {
          if (delta?.content) {
            fullContent += delta.content;
            yield delta.content;
          }
          const rawToolCalls = delta?.tool_calls || (delta?.tool_call ? [delta.tool_call] : []);
          if (rawToolCalls.length) {
            for (const raw of rawToolCalls) {
              processToolCallFragment(context, raw, provider);
            }
            while (context.toolCalls.length > 0) {
              const call = context.toolCalls.shift();
              toolCalls.push(call);
              yield { type: "tool_call", call };
            }
          }
        }
      } catch (err) {
        console.error(`[${provider}] [SSE] Erreur parsing payload: ${err.message}`, {
          payloadPreview: payload.slice(0, 200),
        });
      }
    }
  }

  return {
    content: fullContent,
    toolCalls: toolCalls.length > 0 ? toolCalls : undefined,
  };
}

function handleDirectResponse(data, provider) {
  if (provider === "anthropic") {
    let content = "";

    // Check for thinking blocks
    if (data.thinking && Array.isArray(data.thinking)) {
      const thinkingContent = data.thinking.map((t) => t.content || t.text || "").join("\n");
      if (thinkingContent) {
        content += `<Think>${thinkingContent}</Think>\n\n`;
      }
    }

    // Add regular content
    content += data.content[0].text;

    return {
      content,
      toolCalls: data.tool_uses || [],
    };
  }
}

// Replace previous normalizeToolCall definition:
const normalizeToolCall = (call, idx = 0) => {
  // Accept multiple possible shapes and extract function-like properties
  const fnShape = call.function || call.tool || call.action || call.intent || call.metadata || {};
  let name =
    fnShape.name ||
    call.name ||
    call.tool?.name ||
    call.action?.name ||
    call.intent?.name ||
    call.metadata?.name ||
    "";
  let args = fnShape.arguments ?? call.arguments ?? call.params ?? call.payload ?? "{}";

  if (args == null) args = "{}";
  if (typeof args !== "string") {
    try {
      args = JSON.stringify(args);
    } catch {
      args = String(args);
    }
  }

  // Heuristic inference for missing function name
  if (!name || !name.trim()) {
    try {
      const parsedArgs = JSON.parse(args || "{}");
      if (parsedArgs && typeof parsedArgs === "object") {
        if (parsedArgs.query) {
          name = "web_search";
        }
        // Add additional heuristics here as needed
      }
    } catch {
      // ignore
    }
  }

  name = (name || "").trim();

  return {
    id: call.id || `tool-${Date.now()}-${idx}`,
    type: "function",
    function: {
      name,
      arguments: args,
    },
  };
};
const normalizeToolCalls = (calls = []) => calls.map(normalizeToolCall);

// New helper: assemble tool call fragments and push complete calls to toolCalls
function processToolCallFragment(context, raw, provider) {
  const { pendingToolArgs, pushedToolIds, toolCalls } = context;
  context.toolFragmentCounter = context.toolFragmentCounter || 0;

  const id =
    raw.id ||
    raw.tool_call_id ||
    raw.tool_call?.id ||
    `tool-${Date.now()}-${context.toolFragmentCounter++}`;

  const fn = raw.function || raw.tool || raw.tool_call || raw;
  const name = fn?.name || "";
  let argsFragment = fn?.arguments ?? fn?.args ?? fn?.arguments_text ?? "";

  if (argsFragment === undefined || argsFragment === null) {
    argsFragment = "";
  } else if (typeof argsFragment !== "string") {
    try {
      argsFragment = JSON.stringify(argsFragment);
    } catch {
      argsFragment = String(argsFragment);
    }
  }

  const existing = pendingToolArgs.get(id) || { name: "", argsStr: "" };
  const combinedName = existing.name || name;
  const combinedArgsStr = existing.argsStr + argsFragment;

  pendingToolArgs.set(id, { name: combinedName, argsStr: combinedArgsStr });

  // Try to parse the combined string as JSON only if it looks like JSON
  let parsedArgs;
  try {
    const trimmedArgs = combinedArgsStr.trim();
    if (trimmedArgs.startsWith("{") || trimmedArgs.startsWith("[")) {
      parsedArgs = JSON.parse(trimmedArgs);
    }
  } catch {
    parsedArgs = null; // Not complete / invalid JSON yet
  }

  // If parsed and not already pushed
  if (parsedArgs !== undefined && parsedArgs !== null && !pushedToolIds.has(id)) {
    // Infer a name if missing
    let finalName = combinedName || "";
    if (!finalName && parsedArgs && typeof parsedArgs === "object") {
      if (parsedArgs.query) finalName = "web_search";
      // Add more heuristics here if needed
    }

    if (finalName && TOOL_HANDLERS[finalName]) {
      const fullCall = {
        id,
        type: "function",
        function: {
          name: finalName,
          arguments: JSON.stringify(parsedArgs),
        },
      };
      toolCalls.push(fullCall);
      pushedToolIds.add(id);
      pendingToolArgs.delete(id);
    } else {
      // Mark as pushed/handled so we don't loop forever on fragments
      pushedToolIds.add(id);
      pendingToolArgs.delete(id);
      console.warn(
        `[${provider}] Outil ignorÃ© aprÃ¨s assemblage : ${finalName || "(no-name)"} (id=${id})`
      );
    }
  }
}

// ============================================================================
// ANALYSE DES DIRECTIVES - Extraction des directives utilisateur
// ============================================================================

const MODEL_DIRECTIVE_REGEX = /model\s*=\s*([^\s;]+)/i;
const PROVIDER_DIRECTIVE_REGEX = /provider\s*=\s*(anthropic|openai|huggingface|mistral|google)/i;
const MODE_DIRECTIVE_REGEX = /mode\s*=\s*(debug)/i;
const DB_URL_DIRECTIVE_REGEX = /db(?:_url)?\s*=\s*([^\s;]+)/i;

const MODEL_PROVIDER_PATTERNS = {
  anthropic: ["claude", "anthropic"],
  openai: ["gpt-", "gpt", "openai", "oai"],
  mistral: ["mistral"],
  huggingface: ["huggingface", "hf"],
  google: ["gemini", "google", "goog"],
};
const PROVIDERS = ["openai", "mistral", "huggingface", "anthropic", "google"];

const parseDirectives = (rawQuestion = "") => {
  const trimmed = String(rawQuestion).trim();
  const semicolonIndex = trimmed.indexOf(";");
  const directiveSource = semicolonIndex >= 0 ? trimmed.slice(0, semicolonIndex).trim() : trimmed;
  let userQuestion = semicolonIndex >= 0 ? trimmed.slice(semicolonIndex + 1).trim() : trimmed;

  if (semicolonIndex < 0) {
    userQuestion = userQuestion
      .replace(MODE_DIRECTIVE_REGEX, "")
      .replace(MODEL_DIRECTIVE_REGEX, "")
      .replace(PROVIDER_DIRECTIVE_REGEX, "")
      .replace(DB_URL_DIRECTIVE_REGEX, "")
      .replace(MODEL_MODE_DIRECTIVE_REGEX, "")
      .trim();
  }

  const modelModeMatch = directiveSource.match(MODEL_MODE_DIRECTIVE_REGEX);
  const providerMatch = directiveSource.match(PROVIDER_DIRECTIVE_REGEX);
  const modelMatch = directiveSource.match(MODEL_DIRECTIVE_REGEX);
  const dbUrlMatch = directiveSource.match(DB_URL_DIRECTIVE_REGEX);

  return {
    rawDirective: directiveSource,
    userQuestion,
    hasDirectiveBlock: semicolonIndex >= 0,
    directiveModelMode: modelModeMatch ? modelModeMatch[1].toLowerCase() : null,
    directiveProvider: providerMatch ? providerMatch[1].toLowerCase() : null,
    directiveModel: modelMatch ? modelMatch[1].toLowerCase() : null,
    directiveDbUrl: dbUrlMatch ? dbUrlMatch[1] : null,
  };
};

const detectModelProvider = (model) => {
  if (!model) return null;
  const target = model.toLowerCase();
  return PROVIDERS.find((provider) =>
    MODEL_PROVIDER_PATTERNS[provider]?.some((pattern) => target.includes(pattern))
  );
};

const PROVIDER_ENV_CHECKERS = {
  anthropic: () => Boolean(getConfig("ANTHROPIC_API_KEY")),
  openai: () => Boolean(getConfig("OPENAI_API_KEY")),
  mistral: () => Boolean(getConfig("MISTRAL_API_KEY")),
  huggingface: () => Boolean(getConfig("HUGGINGFACE_API_KEY")),
  google: () => Boolean(getConfig("GEMINI_API_KEY")),
};
const isProviderAvailable = (provider) => Boolean(PROVIDER_ENV_CHECKERS[provider]?.());

const isMistralCapacityError = (error) => {
  const msg = error?.message || "";
  return /service_tier_capacity_exceeded|capacity|3505|429/i.test(msg);
};

const SHOULD_RANDOMIZE_PROVIDERS = true; // TODO: jhr, was: getConfig("DISABLE_PROVIDER_RANDOMIZATION") !== "1";
const shuffleProviders = (providers) => {
  const arr = [...providers];
  for (let i = arr.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [arr[i], arr[j]] = [arr[j], arr[i]];
  }
  return arr;
};

const shouldSkipProvider = ({
  provider,
  modelMode = null,
  enforcedProvider = null,
  resolvedModel = null,
  quiet = false,
}) => {
  try {
    if (enforcedProvider && provider === enforcedProvider) {
      return false; // Respecter le choix explicite de l'utilisateur
    }

    const modelName = resolvedModel || resolveModelForProvider(provider, modelMode);
    if (!modelName) return false;

    const skip = providerMetrics.shouldSkip(provider, modelName);
    if (skip && !quiet) {
      const entry = providerMetrics.get(provider, modelName);
      const status = entry?.status || "unknown";
      const consecutiveErrors = entry?.metrics?.consecutiveErrors || 0;
      const lastErrorMessage = entry?.metrics?.lastError?.message;
      const reason = lastErrorMessage || `${consecutiveErrors} consecutive errors`;
      console.log(
        `[EdgeFunction] â­ï¸ Skipping ${provider} (${modelName}) due to ${status}${
          reason ? ` â€“ ${reason}` : ""
        }`
      );
    }
    return skip;
  } catch (err) {
    console.warn(
      `[EdgeFunction] âš ï¸ Unable to consult provider metrics for ${provider}:`,
      err?.message || err
    );
    return false;
  }
};

const buildProviderOrder = ({
  enforcedProvider = null,
  failedProviders = new Set(),
  modelMode = null,
} = {}) => {
  const order = [...PROVIDERS];
  let prioritizedOrder;

  if (enforcedProvider && order.includes(enforcedProvider)) {
    prioritizedOrder = [enforcedProvider, ...order.filter((p) => p !== enforcedProvider)];
  } else if (!failedProviders.has("openai") && order.includes("openai")) {
    // Prioriser OpenAI si non Ã©chouÃ©
    prioritizedOrder = ["openai", ...order.filter((p) => p !== "openai")];
  } else {
    prioritizedOrder = order;
  }

  const filteredOrder = prioritizedOrder.filter(
    (provider) => !shouldSkipProvider({ provider, modelMode, enforcedProvider, quiet: true })
  );

  // Si tous les providers ont Ã©tÃ© filtrÃ©s, retomber sur l'ordre priorisÃ© initial
  return filteredOrder.length > 0 ? filteredOrder : prioritizedOrder;
};

const parseRetryAfter = (errorMessage) => {
  const match = errorMessage.match(/Please try again in (\d+(?:\.\d+)?)s/);
  return match ? parseFloat(match[1]) * 1000 : 5000; // default 5s if not found
};

const isRateLimitError = (error) => {
  const msg = error?.message || "";
  return /rate.?limit|429/i.test(msg) && /tokens?|requests?/i.test(msg);
};

function createDebugLogger() {
  const pendingLogs = [];
  let controllerRef = null;
  let encoderRef = null;
  let enabled = false;
  const originals = {};

  const formatArgs = (args) =>
    args.map((arg) => (typeof arg === "string" ? arg : JSON.stringify(arg))).join(" ");

  const safeEnqueue = (line) => {
    if (!controllerRef || !encoderRef) {
      pendingLogs.push(line);
      return;
    }
    try {
      controllerRef.enqueue(encoderRef.encode(`\n\n${line}\n\n`));
    } catch {
      // Controller may be closed; fallback to pending logs and detach controller
      pendingLogs.push(line);
      controllerRef = null;
      encoderRef = null;
    }
  };

  const emit = (level, args) => {
    const line = `[DEBUG] ${level.toUpperCase()}: ${formatArgs(args)}`;
    safeEnqueue(line);
  };

  const wrap =
    (level) =>
    (...args) => {
      originals[level](...args);
      emit(level, args);
    };

  return {
    enable() {
      if (enabled) return;
      enabled = true;
      originals.log = console.log;
      originals.warn = console.warn;
      originals.error = console.error;
      console.log = wrap("log");
      console.warn = wrap("warn");
      console.error = wrap("error");
    },
    attachStream(controller, encoder) {
      if (!enabled) return;
      controllerRef = controller;
      encoderRef = encoder;
      if (pendingLogs.length > 0) {
        // try to flush, keep safe if controller throws
        const logsToFlush = pendingLogs.splice(0);
        for (const line of logsToFlush) {
          try {
            controller.enqueue(encoder.encode(`\n\n${line}\n\n`));
          } catch {
            // If fails, put the remaining lines back to pending logs
            pendingLogs.unshift(line);
            controllerRef = null;
            encoderRef = null;
            break;
          }
        }
      }
    },
    disable() {
      if (!enabled) return;
      console.log = originals.log;
      console.warn = originals.warn;
      console.error = originals.error;
      enabled = false;
      controllerRef = null;
      encoderRef = null;
    },
  };
}

// ============================================================================
// SYSTEM PROMPT - Chargement dynamique
// ============================================================================

async function fetchPublicSystemPrompt(siteUrl) {
  if (!siteUrl) return null;

  const promptFiles = ["bob-system.md", "bob-db-capabilities.md"];
  const collected = [];

  for (const fileName of promptFiles) {
    const promptUrl = `${siteUrl}/prompts/${fileName}`;
    try {
      console.log(`[Prompt] âžœ fetching system prompt from ${promptUrl}`);
      const response = await fetch(promptUrl);
      console.log(`[Prompt] â¬… ${fileName} status=${response.status}`);
      if (!response.ok) continue;

      const content = await response.text();
      console.log(`[Prompt] â¬… ${fileName} length=${content.length}`);
      if (content.trim()) {
        collected.push(`<!-- ${fileName} -->\n${content.trim()}`);
      }
    } catch (error) {
      console.warn(`[SystemPrompt] Erreur fetch ${fileName}:`, error.message);
    }
  }

  if (collected.length === 0) return null;
  return collected.join("\n\n---\n\n");
}

async function _fetchCouncilContext(siteUrl) {
  if (!siteUrl) return null;
  try {
    const councilUrl = `${siteUrl}/docs/conseils/conseil-consolidated.semantic.md`;
    console.log(`[Council] âžœ fetching consolidated council context from ${councilUrl}`);
    const response = await fetch(councilUrl);
    console.log(`[Council] â¬… status=${response.status}`);
    if (response.ok) {
      const text = await response.text();
      console.log(`[Council] â¬… content length=${text.length}`);
      if (text.trim()) return text;
    }
  } catch (error) {
    console.warn("[Council] âŒ Unable to fetch consolidated council context:", error.message);
  }
  return null;
}

async function getSystemPrompt() {
  const currentDate = new Date().toLocaleDateString("fr-FR", {
    weekday: "long",
    year: "numeric",
    month: "long",
    day: "numeric",
  });
  let basePrompt = `ðŸ“… **Date actuelle :** ${currentDate}\n\n`;

  // 1. Charge le prompt depuis l'URL publique
  // TODO: fix key + should use localhost when appropriate
  const siteUrl = getConfig("URL") || getConfig("DEPLOY_PRIME_URL");
  const localPrompt = await fetchPublicSystemPrompt(siteUrl);
  if (localPrompt) {
    basePrompt += localPrompt;
  } else {
    // 2. Fallback avec les variables d'environnement
    const envPrompt = getConfig("BOB_SYSTEM_PROMPT");
    if (envPrompt) {
      basePrompt += envPrompt;
    } else {
      // 3. Fallback par dÃ©faut
      console.warn("Bad configuration, using default prompt.");
      const city = getConfig("CITY_NAME") || "Corte";
      const movement = getConfig("MOVEMENT_NAME") || "Pertitellu";
      const bot = getConfig("BOT_NAME") || "OphÃ©lia";
      basePrompt += `
      **RÃ´le :** Tu es **${bot}**, l'assistant citoyen du mouvement **${movement}** pour la commune de **${city}**.

      **Instructions :**
      - RÃ©ponds **uniquement en franÃ§ais**, de maniÃ¨re **factuelle, concise et structurÃ©e** (Markdown : titres, listes, liens).
      - Cite toujours tes **sources officielles** quand c'est possible.
      - Pour les questions locales (projets, horaires), utilise les outils disponibles (**web_search**, **vector_search** pour l'histoire locale).
      - Si tu ne connais pas la rÃ©ponse, dis-le clairement et propose une alternative.

      **Exemple de rÃ©ponse :**
      > **Horaires de la mairie :**
      > - Lundi-vendredi : 8h30-17h
      > - Samedi : 9h-12h
      > *(Source : [site de la mairie](#))*`;
    }
  }

  // 4. Charge le wiki consolidÃ© depuis Supabase
  /* JHR 2024-06-10 : dÃ©sactivÃ© pour l'instant car trop volumineux et ralentit tout le systÃ¨me
    const supabaseUrl = getConfig("SUPABASE_URL");
    const supabaseKey = getConfig("SUPABASE_SERVICE_ROLE_KEY");
    if (supabaseUrl && supabaseKey) {
      try {
        const response = await fetch(
          `${supabaseUrl}/rest/v1/consolidated_wiki_documents?select=content&order=updated_at.desc&limit=1`,
          {
            headers: {
              apikey: supabaseKey,
              Authorization: `Bearer ${supabaseKey}`,
              "Content-Type": "application/json",
            },
          }
        );
        if (response.ok) {
          const data = await response.json();
          console.log(
            `[SystemPrompt] Supabase data preview: ${previewForLog(data?.[0]?.content, 100)}`
          );
          if (data?.length > 0 && data[0].content) {
            basePrompt += `\n\nðŸ“š **Contexte local (wiki) :**\n${data[0].content}...`;
          }
        }
      } catch (error) {
        console.error("[SystemPrompt] Erreur Supabase:", error.message);
      }
    }
    */

  // 5. Charge le contexte municipal (si disponible)
  /* JHR 2024-06-10 : dÃ©sactivÃ© pour l'instant car trop volumineux et ralentit tout le systÃ¨me
    const councilContext = await _fetchCouncilContext(siteUrl);
    if (councilContext) {
      basePrompt += `\n\nðŸ› **Contexte municipal (conseils consolidÃ©s) :**\n${councilContext}...`;
    } else {
      basePrompt += `\n\nðŸ› **Contexte municipal (conseils consolidÃ©s) :** indisponible pour le moment.`;
    }
    */
  console.log(`[SystemPrompt] âœ… Prompt chargÃ© (${basePrompt.length} caractÃ¨res)`);
  return basePrompt;
}

// ============================================================================
// HANDLER - Fonction principale de gestion des requÃªtes
// ============================================================================

const handler = async (request) => {
  // Load instance config from supabase
  await loadInstanceConfig();
  // Defensive: a supabase client should be available
  if (!getSupabase()) {
    console.warn("loadInstanceConfig: supabase client not available, fatal");
    throw new Error("loadInstanceConfig: supabase client not available, fatal");
  }
  // Quick healthcheck support (frontend calls GET /api/chat-stream?healthcheck=true)
  try {
    const url = new URL(request.url);
    if (request.method === "GET" && url.searchParams.get("healthcheck") === "true") {
      const providersList = (PROVIDERS || []).map((p) => {
        const configured = isProviderAvailable(p);
        const model = resolveModelForProvider(p);
        return {
          name: p,
          status: configured ? "available" : "not_configured",
          models: [
            {
              name: model || null,
              avgResponseTime: null,
              successRate: null,
              recentlyUsed: false,
              retryAfter: null,
              consecutiveErrors: 0,
            },
          ],
        };
      });
      return new Response(JSON.stringify({ providers: providersList }), {
        status: 200,
        headers: { "Content-Type": "application/json" },
      });
    }
  } catch {
    // continue to normal handler on malformed URL
  }

  // 1. VÃ©rifie la mÃ©thode HTTP
  if (request.method !== "POST") {
    return new Response(JSON.stringify({ error: "MÃ©thode non autorisÃ©e." }), { status: 405 });
  }

  // 2. Parse le corps de la requÃªte
  let body;
  try {
    body = await request.json();
  } catch {
    return new Response("Charge utile invalide", { status: 400 });
  }

  // Support POST-based healthcheck bodies: { healthcheck: true } or question === 'healthcheck'
  try {
    if (
      body &&
      (body.healthcheck === true || String(body.question || "").toLowerCase() === "healthcheck")
    ) {
      const providersList = (PROVIDERS || []).map((p) => {
        const configured = isProviderAvailable(p);
        const model = resolveModelForProvider(p);
        return {
          name: p,
          status: configured ? "available" : "not_configured",
          models: [
            {
              name: model || null,
              avgResponseTime: null,
              successRate: null,
              recentlyUsed: false,
              retryAfter: null,
              consecutiveErrors: 0,
            },
          ],
        };
      });
      return new Response(JSON.stringify({ providers: providersList }), {
        status: 200,
        headers: { "Content-Type": "application/json" },
      });
    }
  } catch {
    // ignore and continue
  }

  // 3. Valide la question
  // Early explicit SQL handling: allow `?sql=` or `body.sql` to run without a `question` field.
  try {
    const { handleExplicitSql } = await import("./lib/sql-handler.js");
    const sqlResp = await handleExplicitSql(request, body, TOOL_HANDLERS);
    if (sqlResp) return sqlResp;
  } catch (err) {
    console.warn("[EdgeFunction] âš ï¸ Early SQL helper error:", err?.message || err);
  }
  const rawQuestion = String(body?.question || "").trim();
  if (!rawQuestion) {
    return new Response("Question manquante", { status: 400 });
  }

  // 4. RÃ©cupÃ¨re et normalise l'historique de conversation (accepte plusieurs formats)
  let conversation_history = [];
  const rawConvCandidates = [
    body?.conversation_history,
    body?.conversationHistory,
    body?.history,
    body?.messages,
    body?.conversation,
  ];
  for (const candidate of rawConvCandidates) {
    if (!candidate) continue;
    if (Array.isArray(candidate)) {
      conversation_history = candidate.slice();
      break;
    }
    if (typeof candidate === "string") {
      // Try JSON parse first
      try {
        const parsed = JSON.parse(candidate);
        if (Array.isArray(parsed)) {
          conversation_history = parsed;
          break;
        }
      } catch {
        // Not JSON: fall back to newline-splitting into user messages
        const lines = candidate
          .split(/\r?\n/)
          .map((s) => s.trim())
          .filter(Boolean);
        if (lines.length > 0) {
          conversation_history = lines.map((l) => ({ role: "user", content: l }));
          break;
        }
      }
    }
  }

  // Ensure normalized structure: array of {role, content} and strip <Think> blocks from assistant messages
  conversation_history = conversation_history.map((m) => {
    if (!m) return { role: "user", content: "" };
    if (typeof m === "string") return { role: "user", content: m };
    let content = String(m.content || "");
    if (m.role === "assistant") {
      // Remove all <Think> blocks from history sent to LLM
      content = content.replace(/<Think>[\s\S]*?<\/Think>/gi, "").trim();
    }
    return { role: m.role || "user", content };
  });

  // Diagnostic logging to help frontend debugging: show counts and sample
  try {
    const totalChars = conversation_history.reduce((s, m) => s + String(m.content || "").length, 0);
    const first = conversation_history
      .slice(0, 3)
      .map((m) => ({ role: m.role, preview: String(m.content || "").slice(0, 200) }));
    const last = conversation_history
      .slice(-3)
      .map((m) => ({ role: m.role, preview: String(m.content || "").slice(0, 200) }));
    console.log(
      `[EdgeFunction] ðŸ“š Historique: ${conversation_history.length} messages, totalChars=${totalChars}`
    );
    console.log(`[EdgeFunction] ðŸ“š Sample first: ${JSON.stringify(first)}`);
    console.log(`[EdgeFunction] ðŸ“š Sample last: ${JSON.stringify(last)}`);
  } catch (err) {
    console.warn("[EdgeFunction] âš ï¸ Failed to log conversation sample:", err?.message || err);
  }

  // 5. Parse les directives (modÃ¨le, fournisseur, debug)
  const {
    rawDirective,
    userQuestion,
    directiveModelMode,
    directiveProvider,
    directiveModel,
    directiveDbUrl,
  } = parseDirectives(rawQuestion);

  const bodyModelMode =
    typeof body?.modelMode === "string" ? body.modelMode.trim().toLowerCase() : null;
  const effectiveModelMode = directiveModelMode || bodyModelMode;
  const debugMode = Boolean(rawDirective && MODE_DIRECTIVE_REGEX.test(rawDirective));

  // 6. DÃ©termine le fournisseur et le modÃ¨le
  const forcedProvider = directiveProvider; // Ex: "provider=anthropic"
  const modelProvider = directiveModel ? detectModelProvider(directiveModel) : null;

  // 7. VÃ©rifie la disponibilitÃ© des clÃ©s API
  if (forcedProvider && !isProviderAvailable(forcedProvider)) {
    return new Response(
      JSON.stringify({
        error: `Le fournisseur "${forcedProvider}" est demandÃ© mais non configurÃ©.`,
      }),
      { status: 400 }
    );
  }

  if (modelProvider && !isProviderAvailable(modelProvider)) {
    return new Response(
      JSON.stringify({
        error: `Le modÃ¨le "${directiveModel}" requiert "${modelProvider}", mais sa clÃ© API est absente.`,
      }),
      { status: 400 }
    );
  }

  // 8. DÃ©termine l'ordre des fournisseurs
  const enforcedProvider = forcedProvider || modelProvider;
  const failedProviders = new Set(); // Suivi des Ã©checs pendant la conversation
  let providerOrder = buildProviderOrder({
    enforcedProvider,
    failedProviders,
    modelMode: effectiveModelMode,
  });
  if (!enforcedProvider && SHOULD_RANDOMIZE_PROVIDERS) {
    providerOrder = shuffleProviders(providerOrder);
  }
  console.log(
    `[EdgeFunction] ðŸ”§ Fournisseur: ${enforcedProvider || "auto"} (ordre=${providerOrder.join(",")})`
  );

  // 9. Active les logs de debug
  const debugLogger = debugMode ? createDebugLogger() : null;
  debugLogger?.enable();

  // 10. Logs initiaux
  console.log(`[EdgeFunction] ========================================`);
  console.log(`[EdgeFunction] ðŸŽ¯ Question: "${rawQuestion}"`);
  console.log(`[EdgeFunction] ðŸ“š Historique: ${conversation_history.length} messages`);
  console.log(`[EdgeFunction] ðŸ”§ Fournisseur: ${enforcedProvider || "auto"}`);
  console.log(`[EdgeFunction] â±ï¸ DÃ©but: ${new Date().toISOString()}`);

  // 11. Charge le prompt systÃ¨me
  let systemPrompt = await getSystemPrompt();
  console.log(`[EdgeFunction] ðŸ“ System prompt: ${systemPrompt.length} caractÃ¨res`);

  // 11.5. Initialise les clients
  let user = null;
  const context = body?.context || {}; // Extraire le contexte utilisateur si prÃ©sent
  const authHeader = request.headers.get("Authorization");
  const supabaseUrl = getConfig("SUPABASE_URL");
  const supabaseAnonKey = getConfig("SUPABASE_ANON_KEY");

  if (authHeader && supabaseUrl && supabaseAnonKey) {
    try {
      const token = authHeader.replace("Bearer ", "");
      // Create a temporary client to verify the user's token
      const supabaseAuth = createClient(supabaseUrl, supabaseAnonKey, {
        global: { headers: { Authorization: `Bearer ${token}` } },
      });
      const {
        data: { user: authUser },
        error: authError,
      } = await supabaseAuth.auth.getUser();
      if (!authError && authUser) {
        user = authUser;
        console.log(`[EdgeFunction] ðŸ‘¤ Authenticated user: ${user.id}`);
      }
    } catch (e) {
      console.warn("[EdgeFunction] âš ï¸ Error parsing auth header:", e.message);
    }
  }

  // Use the supabase client attached to the instance (usually service role)
  const supabase = getSupabase();

  const openai = new OpenAI({ apiKey: getConfig("OPENAI_API_KEY") });

  // 11.6. Retrieve local vector-search context and append to system prompt
  try {
    const vectorContext = await TOOL_HANDLERS.vector_search(
      { query: userQuestion, limit: 5 },
      { supabase, openai }
    );
    if (vectorContext && typeof vectorContext === "string" && vectorContext.trim()) {
      // Keep inserted context concise to avoid prompt bloat
      const truncated =
        vectorContext.length > 4000
          ? vectorContext.slice(0, 4000) + "\n... (truncated)"
          : vectorContext;
      systemPrompt += `\n\nðŸ“š Connaissances locales (extrait) :\n${truncated}\n\n`;
      console.log(
        `[EdgeFunction] â„¹ï¸ Appended vector-search context (${String(truncated).length} chars) to system prompt`
      );
    }
  } catch (err) {
    console.warn("[EdgeFunction] âš ï¸ vector_search failed:", err?.message || err);
  }

  // 12. CrÃ©e un ReadableStream pour la rÃ©ponse
  const encoder = new TextEncoder();
  const readable = new ReadableStream({
    async start(controller) {
      debugLogger?.attachStream(controller, encoder);
      const emitProviderMeta = (meta) =>
        controller.enqueue(encoder.encode(`${PROVIDER_META_PREFIX}${JSON.stringify(meta)}\n`));
      const emitThink = (message) => {
        const text = String(message || "").trim();
        if (!text) return;
        const safe = text.replaceAll("<Think>", "").replaceAll("</Think>", "");
        controller.enqueue(encoder.encode(`<Think>${safe}</Think>\n`));
      };
      const emitToolTrace = (trace) => {
        if (!trace) return;
        try {
          controller.enqueue(encoder.encode(`${TOOL_TRACE_PREFIX}${JSON.stringify(trace)}\n`));
          if (trace.phase === "finish") {
            const dur = Number.isFinite(trace.durationMs) ? `${trace.durationMs}ms` : null;
            emitThink(
              `Outil terminÃ© : ${trace.tool}${dur ? ` (${dur})` : ""}${
                trace.resultPreview ? ` â€” ${previewForLog(trace.resultPreview, 160)}` : ""
              }`
            );
          } else if (trace.phase === "error") {
            emitThink(
              `Erreur outil : ${trace.tool}${trace.error ? ` â€” ${previewForLog(trace.error, 200)}` : ""}`
            );
          } else if (trace.phase === "notice" && trace.message) {
            emitThink(previewForLog(trace.message, 220));
          }
        } catch (err) {
          console.warn("[EdgeFunction] âš ï¸ Failed to emit tool trace:", err?.message || err);
        }
      };

      // PrÃ©fixes pour les logs
      const _logPrefix = "ðŸ“œ [LOG] ";
      const errorPrefix = "âŒ [ERREUR] ";
      const chunkPrefix = "";

      let handled = false;
      const lastError = null;

      // 13. Essaie chaque fournisseur dans l'ordre
      emitThink(
        `Ordre des fournisseurs : ${providerOrder.join(", ")}${
          enforcedProvider
            ? ` (forcÃ©=${enforcedProvider})`
            : SHOULD_RANDOMIZE_PROVIDERS
              ? " (alÃ©atoire)"
              : ""
        }`
      );
      for (let providerIndex = 0; providerIndex < providerOrder.length; providerIndex++) {
        const provider = providerOrder[providerIndex];
        const resolvedModel = resolveModelForProvider(provider, effectiveModelMode);

        const skip = shouldSkipProvider({
          provider,
          modelMode: effectiveModelMode,
          enforcedProvider,
          resolvedModel,
        });
        if (skip) {
          try {
            const modelName =
              resolvedModel || resolveModelForProvider(provider, effectiveModelMode);
            const entry = modelName ? providerMetrics.get(provider, modelName) : null;
            const status = entry?.status || "unknown";
            const consecutiveErrors = entry?.metrics?.consecutiveErrors || 0;
            const lastErrorMessage = entry?.metrics?.lastError?.message;
            emitThink(
              `Saut du fournisseur ${provider}${modelName ? ` (${modelName})` : ""}: ${status}${
                lastErrorMessage
                  ? ` â€” ${previewForLog(lastErrorMessage, 160)}`
                  : consecutiveErrors
                    ? ` â€” ${consecutiveErrors} erreurs consÃ©cutives`
                    : ""
              }`
            );
          } catch {
            emitThink(`Saut du fournisseur ${provider}`);
          }
          continue;
        }

        let providerRetries = 0;
        const maxProviderRetries = 2;

        while (providerRetries <= maxProviderRetries) {
          try {
            // GESTION SPÃ‰CIFIQUE POUR LA CLÃ‰ API GEMINI
            let apiKey;
            if (provider === "google") {
              apiKey = getConfig("GEMINI_API_KEY");
            } else {
              apiKey = getConfig(`${provider.toUpperCase()}_API_KEY`);
            }
            if (!apiKey) {
              console.log(`[EdgeFunction] â­ï¸ Skipping ${provider} (no API key)`);
              emitThink(`Saut du fournisseur ${provider} : clÃ© API manquante`);
              // Mark provider as failed/unavailable so we don't retry indefinitely
              try {
                failedProviders.add(provider);
              } catch (_) {
                /* ignored */
              }
              // break the retry loop to move to the next provider
              break;
            }
            console.log(
              `[EdgeFunction] ðŸ” Model resolution: provider=${provider}, mode=${effectiveModelMode}, resolved=${resolvedModel}`
            );
            console.log(
              `[EdgeFunction] ðŸ” Available modes for ${provider}:`,
              MODEL_MODES[provider]
            );
            emitProviderMeta({ provider, model: resolvedModel });
            console.log(`[EdgeFunction] ðŸš€ Tentative avec ${provider} (model=${resolvedModel})...`);
            emitThink(
              `Tentative avec le fournisseur ${provider}${resolvedModel ? ` (${resolvedModel})` : ""}${
                enforcedProvider ? ` â€” forcÃ©=${enforcedProvider}` : ""
              }`
            );
            if (provider === "huggingface") {
              // HuggingFace a une API diffÃ©rente (non-streaming)
              const result = await runHuggingFaceAgent(
                userQuestion,
                systemPrompt,
                effectiveModelMode
              );
              controller.enqueue(encoder.encode(chunkPrefix + String(result)));
            } else {
              // Mistral, OpenAI, Anthropic utilisent tous runConversationalAgent
              const agentMeta = {};
              const providerAttemptStart = Date.now();
              for await (const chunk of runConversationalAgent({
                provider,
                question: userQuestion,
                systemPrompt,
                conversationHistory: conversation_history,
                maxToolCalls: 2,
                modelMode: effectiveModelMode,
                supabase,
                openai,
                metaCollector: agentMeta,
                toolTraceEmitter: emitToolTrace,
                debugMode,
                user, // Pass authenticated user
                context: body.context || {}, // Pass frontend context
              })) {
                // If the generator yields an object, serialize it as provider metadata
                try {
                  if (chunk && typeof chunk === "object") {
                    controller.enqueue(
                      encoder.encode(PROVIDER_META_PREFIX + JSON.stringify(chunk) + "\n")
                    );
                  } else {
                    controller.enqueue(encoder.encode(chunkPrefix + String(chunk)));
                  }
                } catch (err) {
                  console.warn("[EdgeFunction] âš ï¸ Failed to enqueue chunk:", err);
                }
              }
              // Populate and emit agent metadata if populated
              try {
                if (agentMeta) {
                  agentMeta.provider = agentMeta.provider || provider;
                  agentMeta.model = agentMeta.model || resolvedModel;
                  agentMeta.agent_duration_ms = Date.now() - providerAttemptStart;
                  agentMeta.tool_trace = agentMeta.tool_trace || [];
                  emitProviderMeta({ __agent_metadata__: agentMeta });
                }
              } catch (err) {
                console.warn(
                  "[EdgeFunction] âš ï¸ Failed to emit agent metadata:",
                  err?.message || err
                );
              }
            }
            handled = true;
            emitThink(
              `SuccÃ¨s du fournisseur : ${provider}${resolvedModel ? ` (${resolvedModel})` : ""}`
            );
            break;
          } catch (error) {
            const isForcedProvider = forcedProvider === provider;
            const capacityError = provider === "mistral" && isMistralCapacityError(error);
            const rateLimitError = provider === "openai" && isRateLimitError(error);

            if (capacityError && !isForcedProvider) {
              console.warn(
                `[EdgeFunction] âš ï¸ ${provider} capacitÃ© atteinte, passage au fournisseur suivant.`
              );
              emitThink(
                `Changement de fournisseur : ${provider} capacitÃ© dÃ©passÃ©e â€” tentative avec le suivant`
              );
              failedProviders.add(provider);
              break; // Passe immÃ©diatement au provider suivant
            } else if (rateLimitError && providerRetries < maxProviderRetries) {
              const delayMs = parseRetryAfter(error.message);
              console.warn(
                `[EdgeFunction] â³ ${provider} rate limit, retrying in ${delayMs}ms (attempt ${providerRetries + 1}/${maxProviderRetries + 1})`
              );
              emitThink(
                `Nouvelle tentative ${provider} : limite de dÃ©bit atteinte â€” attente de ${Math.round(delayMs)}ms (tentative ${providerRetries + 1}/${maxProviderRetries + 1})`
              );
              await new Promise((resolve) => setTimeout(resolve, delayMs));
              providerRetries++;
              continue; // retry same provider
            } else {
              const errorDetail = error.message || String(error);
              console.error(`[EdgeFunction] âŒ ${provider} error:`, errorDetail);

              // If this is a forced provider, show error to user (no fallback available)
              if (isForcedProvider) {
                emitThink(
                  `Ã‰chec du fournisseur (forcÃ©) : ${provider} â€” pas de repli possible${
                    errorDetail ? ` â€” ${previewForLog(errorDetail, 180)}` : ""
                  }`
                );
                const errorMessage = `âš ï¸ Le fournisseur ${provider} que vous avez demandÃ© n'est pas disponible actuellement.\n\n**DÃ©tails** : ${errorDetail}\n\n`;
                console.error(
                  `[EdgeFunction] ðŸ›‘ Forced provider ${provider} failed, not falling back`
                );
                controller.enqueue(encoder.encode(errorMessage));
                handled = true;
                break;
              }

              // For automatic fallback: log in backend, don't show in UI (unless debug mode)
              console.warn(`[EdgeFunction] âš ï¸ ${provider} failed, trying next provider...`);
              emitThink(
                `Changement de fournisseur : ${provider} a Ã©chouÃ© â€” tentative avec le suivant${
                  errorDetail ? ` â€” ${previewForLog(errorDetail, 160)}` : ""
                }`
              );

              failedProviders.add(provider);
              break; // move to next provider
            }
          }
        }
        if (handled) break;
      }

      // 14. Gestion des erreurs
      if (!handled) {
        const message = `âŒ DÃ©solÃ©, le service est temporairement indisponible.\n\nNos fournisseurs d'IA rencontrent actuellement des difficultÃ©s. Veuillez rÃ©essayer dans quelques instants.\n\n`;
        controller.enqueue(encoder.encode(`${message}`));
      }
      // Emit final providers status (frontend reads metrics from this stream end)
      try {
        const providersList = (PROVIDERS || []).map((provider) => {
          const configured = isProviderAvailable(provider);
          const modelModes = MODEL_MODES[provider] || {};
          const models = Object.entries(modelModes).map(([mode, modelName]) => {
            const metricEntry = providerMetrics.get(provider, modelName);
            const metrics = metricEntry?.metrics || {};
            const successRate =
              metrics.requestCount && metrics.requestCount > 0
                ? Math.round((metrics.successCount / metrics.requestCount) * 100)
                : null;
            let retryAfter = null;
            const lastError = metricEntry?.metrics?.lastError;
            if (metricEntry?.status === "rate_limited" && lastError?.retryAfter) {
              const retryTime = lastError.timestamp + lastError.retryAfter * 1000;
              const secondsUntilRetry = Math.max(0, Math.ceil((retryTime - Date.now()) / 1000));
              if (secondsUntilRetry > 0) retryAfter = secondsUntilRetry;
            }
            return {
              name: modelName,
              mode,
              avgResponseTime: metrics.avgResponseTime ?? null,
              successRate,
              recentlyUsed: Boolean(metrics.lastUsed && Date.now() - metrics.lastUsed < 30000),
              retryAfter,
              consecutiveErrors: metrics.consecutiveErrors || 0,
              status: metricEntry?.status || (configured ? "available" : "not_configured"),
            };
          });

          let providerStatus = "available";
          if (!configured) {
            providerStatus = "not_configured";
          } else if (models.length === 0 || models.every((m) => m.status === "unknown")) {
            providerStatus = "unknown";
          } else if (
            models.some((m) => ["error", "quota_exceeded"].includes((m.status || "").toLowerCase()))
          ) {
            providerStatus = "degraded";
          } else if (models.every((m) => m.status === "rate_limited")) {
            providerStatus = "rate_limited";
          }

          return {
            name: provider,
            status: providerStatus,
            models,
          };
        });
        controller.enqueue(
          encoder.encode(
            `${PROVIDERS_STATUS_PREFIX}${JSON.stringify({ providers: providersList })}\n`
          )
        );
      } catch (err) {
        console.warn("[EdgeFunction] âš ï¸ Failed to emit providers status:", err?.message || err);
      }
      controller.close();
    },

    cancel() {
      debugLogger?.disable();
    },
  });

  // 15. Retourne la rÃ©ponse streamÃ©e
  return new Response(readable, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      "X-Content-Type-Options": "nosniff",
    },
  });
};

// Add runConversationalAgent (hoisted so handler can call it)
async function* runConversationalAgent({
  provider = "mistral",
  question,
  systemPrompt,
  conversationHistory = [],
  maxToolCalls = 2,
  modelMode,
  supabase,
  openai,
  metaCollector = null,
  toolTraceEmitter = null,
  debugMode = false,
  user = null,
  context = {},
}) {
  let toolCallCount = 0;
  const idleTimeoutMs = Number(getConfig("LLM_STREAM_TIMEOUT_MS")) || 30000;
  const agentStartMs = Date.now();

  let messages = [
    { role: "system", content: systemPrompt },
    ...conversationHistory,
    { role: "user", content: question },
  ];

  console.log(`[${provider}] âœ… runConversationalAgent initialized (maxToolCalls=${maxToolCalls})`);
  while (toolCallCount < maxToolCalls) {
    const model = resolveModelForProvider(provider, modelMode);
    console.log(`[${provider}] ðŸ” Appel LLM (model=${model}) - messages:${messages.length}`);
    yield `<Think>Appel LLM : fournisseur=${provider}${model ? ` modÃ¨le=${model}` : ""}, messages=${messages.length}, outilsUtilisÃ©s=${toolCallCount}/${maxToolCalls}</Think>\n`;
    const streamOrDirect = await callLLMAPI({
      provider,
      model,
      messages,
      tools: Object.values(TOOLS),
      toolChoice: "auto",
      stream: true,
    });

    // Diagnostic: capture exact shape returned by callLLMAPI for non-stream cases
    try {
      console.log(
        `[${provider}] DEBUG streamOrDirect typeof=${typeof streamOrDirect} isAsyncIterable=${isAsyncIterable(streamOrDirect)}`
      );
      console.log(
        `[${provider}] DEBUG streamOrDirect preview: ${previewForLog(streamOrDirect, 1000)}`
      );
    } catch (err) {
      console.warn(`[${provider}] âš ï¸ Failed to preview streamOrDirect: ${err?.message || err}`);
    }

    // Direct (non-stream) response
    if (!isAsyncIterable(streamOrDirect)) {
      console.log(`[${provider}] â„¹ï¸ Direct LLM response received`);
      const data = streamOrDirect || {};
      if (data.toolCalls && data.toolCalls.length > 0) {
        const normalized = normalizeToolCalls(data.toolCalls);
        const valid = normalized.filter((c) => c.function?.name && TOOL_HANDLERS[c.function.name]);
        if (valid.length > 0) {
          toolCallCount++;
          console.log(
            `[${provider}] ðŸ›  Executing ${valid.length} tool(s) (direct):`,
            valid.map((c) => c.function.name)
          );
          yield `<Think>Outils demandÃ©s (direct) : ${valid
            .map((c) => c.function?.name)
            .filter(Boolean)
            .join(", ")}</Think>\n`;
          const toolMessages = await executeToolCalls(
            valid,
            provider,
            {
              web_search: { query: question },
              defaultQuery: question,
            },
            supabase,
            openai,
            metaCollector,
            toolTraceEmitter,
            debugMode,
            user,
            context
          );
          yield `<Think>Outils exÃ©cutÃ©s (direct) : ${valid
            .map((c) => c.function?.name)
            .filter(Boolean)
            .join(", ")} â€” reprise du LLM</Think>\n`;
          messages = [
            ...messages,
            {
              role: "assistant",
              content: data.content || null,
              ...(provider === "anthropic" ? { tool_uses: valid } : { tool_calls: valid }),
            },
            ...toolMessages,
          ];
          continue; // re-run LLM with augmented messages
        }
      }
      if (data.content) {
        yield data.content;
      }
      return;
    }

    // Streamed response: iterate events with timeout
    console.log(`[${provider}] ðŸš€ Streaming LLM response - processing events`);
    const iterator = streamOrDirect[Symbol.asyncIterator]?.();
    let accumulatedContent = "";
    let eventToolExecuted = false;
    let streamTimedOut = false;
    let finalStreamResult = undefined;
    let lastStreamToolInfo = null;

    try {
      while (true) {
        const nextPromise = iterator.next();
        let res;
        try {
          res = await Promise.race([
            nextPromise,
            new Promise((_, reject) =>
              setTimeout(() => reject(new Error("stream-timeout")), idleTimeoutMs)
            ),
          ]);
        } catch (err) {
          if (err?.message === "stream-timeout") {
            console.warn(
              `[${provider}] âš ï¸ Stream idle timeout (${idleTimeoutMs}ms). Falling back to direct call.`
            );
            yield `<Think>DÃ©lai d'attente du flux dÃ©passÃ© (${idleTimeoutMs}ms) : passage en mode non-stream</Think>\n`;
            streamTimedOut = true;
            break;
          }
          throw err;
        }

        if (res.done) {
          console.log(`[${provider}] â„¹ï¸ Stream finished cleanly`);
          finalStreamResult = res.value;
          break;
        }
        const evt = res.value;
        if (!evt) continue;

        if (typeof evt === "string") {
          accumulatedContent += evt;
          yield evt;
          continue;
        }
        if (evt.type === "content") {
          accumulatedContent += evt.chunk;
          yield evt.chunk;
          continue;
        }
        if (evt.type === "tool_call") {
          const call = evt.call;
          const fnName = call?.function?.name;
          console.log(
            `[${provider}] ðŸ›  Received tool_call event: id=${call?.id}, name=${fnName || "(no-name)"}`
          );

          if (!fnName || !TOOL_HANDLERS[fnName]) {
            console.warn(
              `[${provider}] âš ï¸ Unknown/unsupported tool: ${fnName || "(no-name)"} - ignoring`
            );
            continue;
          }

          toolCallCount++;
          if (toolCallCount > maxToolCalls) {
            throw new Error(`[${provider}] Limite de ${maxToolCalls} appels d'outils atteinte.`);
          }

          console.log(`[${provider}] ðŸ›  Executing tool now: ${fnName} (id=${call.id})`);
          yield `<Think>Outil demandÃ© (flux) : ${fnName} (id=${call?.id || "n/a"}) â€” exÃ©cution</Think>\n`;
          lastStreamToolInfo = { name: fnName, id: call?.id };
          const toolMessages = await executeToolCalls(
            [call],
            provider,
            {
              web_search: { query: question },
              defaultQuery: question,
            },
            supabase,
            openai,
            metaCollector,
            toolTraceEmitter,
            debugMode,
            user,
            context
          );
          yield `<Think>Outil exÃ©cutÃ© (flux) : ${fnName} (id=${call?.id || "n/a"}) â€” reprise du LLM</Think>\n`;

          messages = [
            ...messages,
            {
              role: "assistant",
              content: accumulatedContent || null,
              ...(provider === "anthropic" ? { tool_uses: [call] } : { tool_calls: [call] }),
            },
            ...toolMessages,
          ];

          eventToolExecuted = true;
          break; // restart LLM with updated messages
        }
      }
    } finally {
      try {
        if (iterator?.return) await iterator.return();
      } catch {
        /* ignore */
      }
    }

    if (eventToolExecuted) {
      console.log(
        `[${provider}] ðŸ”„ Completed a tool call cycle during streaming, restarting LLM loop`
      );
      console.info(
        `[${provider}] â„¹ï¸ Tool ${lastStreamToolInfo?.name || "(unknown)"} terminÃ©, reprise du flux utilisateur (call id=${lastStreamToolInfo?.id || "n/a"}).`
      );
      continue;
    }

    const streamToolCalls = Array.isArray(finalStreamResult?.toolCalls)
      ? normalizeToolCalls(finalStreamResult.toolCalls)
      : [];
    const validStreamCalls = streamToolCalls.filter(
      (c) => c.function?.name && TOOL_HANDLERS[c.function.name]
    );
    if (validStreamCalls.length > 0) {
      toolCallCount++;
      console.log(
        `[${provider}] ðŸ›  Executing ${validStreamCalls.length} tool(s) (stream completion):`,
        validStreamCalls.map((c) => c.function.name)
      );
      const toolMessages = await executeToolCalls(
        validStreamCalls,
        provider,
        {
          web_search: { query: question },
          defaultQuery: question,
        },
        supabase,
        openai,
        metaCollector,
        toolTraceEmitter,
        debugMode,
        user,
        context
      );
      messages = [
        ...messages,
        {
          role: "assistant",
          content: finalStreamResult?.content || null,
          ...(provider === "anthropic"
            ? { tool_uses: validStreamCalls }
            : { tool_calls: validStreamCalls }),
        },
        ...toolMessages,
      ];
      continue;
    }

    if (accumulatedContent && accumulatedContent.trim().length > 0) {
      console.log(
        `[${provider}] âœ… Streaming provided content (${accumulatedContent.length} chars). Returning.`
      );
      return;
    }

    // Fallback: direct call to fetch content/tool_calls if stream timed out or provided nothing
    console.log(
      `[${provider}] âš ï¸ ${streamTimedOut ? "Stream timed out." : "No tool calls/content from stream."} Attempting direct fallback.`
    );
    if (streamTimedOut) {
      yield `<Think>DÃ©lai d'attente du flux dÃ©passÃ© (${idleTimeoutMs}ms) : basculement vers le mode direct</Think>\n`;
    } else {
      yield `<Think>Flux terminÃ© sans contenu ni appel d'outil : basculement vers le mode direct</Think>\n`;
    }
    const direct = await callLLMAPI({
      provider,
      model: resolveModelForProvider(provider, modelMode),
      messages,
      tools: Object.values(TOOLS),
      toolChoice: "auto",
      stream: false,
    });

    // Normalize possible shapes for tool_calls in direct responses.
    // Providers may place tool calls in different locations:
    // - direct.toolCalls or direct.tool_calls
    // - direct.choices[0].message.tool_calls
    // - direct.choices[0].message.function_call (single function)
    // Normalize to `direct.toolCalls` as an array of { id, function: { name, arguments } }.
    try {
      try {
        console.log(`[${provider}] ðŸ” Direct response keys:`, Object.keys(direct || {}));
        console.log(
          `[${provider}] ðŸ” choices[0].message.tool_calls preview:`,
          previewForLog(
            direct?.choices?.[0]?.message?.tool_calls || direct?.choices?.[0]?.tool_calls,
            200
          )
        );
      } catch (e) {
        /* ignore preview errors */
      }

      const directResp = { ...(direct || {}) };
      // Top-level aliases
      if (Array.isArray(directResp.toolCalls) && directResp.toolCalls.length > 0) {
        // already normalized
      } else if (Array.isArray(directResp.tool_calls) && directResp.tool_calls.length > 0) {
        directResp.toolCalls = directResp.tool_calls;
      } else if (Array.isArray(directResp.choices) && directResp.choices.length > 0) {
        const choice = directResp.choices[0];
        const message = choice?.message || choice || {};

        // If tool_calls array is present on the message/choice, use it
        const candidateArray =
          message?.tool_calls || message?.toolCalls || choice?.tool_calls || choice?.toolCalls;
        if (Array.isArray(candidateArray) && candidateArray.length > 0) {
          directResp.toolCalls = candidateArray;
        } else if (
          message?.function_call &&
          (message.function_call.name || message.function_call?.id)
        ) {
          // Single function_call -> convert to toolCalls array
          directResp.toolCalls = [
            {
              id: choice?.id || `call-${Date.now()}`,
              function: {
                name: message.function_call.name || message.function_call?.id || "",
                arguments: message.function_call.arguments || "{}",
              },
            },
          ];
        } else if (
          choice?.function_call &&
          (choice.function_call.name || choice.function_call.arguments)
        ) {
          directResp.toolCalls = [
            {
              id: choice?.id || `call-${Date.now()}`,
              function: {
                name: choice.function_call.name || "",
                arguments: choice.function_call.arguments || "{}",
              },
            },
          ];
        }
      }
      // Ensure toolCalls is an array if present
      if (directResp.toolCalls && !Array.isArray(directResp.toolCalls)) {
        directResp.toolCalls = [directResp.toolCalls];
      }
      // Optional: surface the normalized payload for diagnostics
      if (directResp.toolCalls && Array.isArray(directResp.toolCalls)) {
        console.log(
          `[${provider}] ðŸ”§ Normalized direct.toolCalls:`,
          directResp.toolCalls.map((c) => ({ id: c.id, name: c.function?.name }))
        );
      }

      const directHasContent = Boolean(directResp?.content && String(directResp.content).trim());
      const directHasToolCalls =
        Array.isArray(directResp?.toolCalls) && directResp.toolCalls.length > 0;

      if (directHasToolCalls) {
        const normalized = normalizeToolCalls(directResp.toolCalls);
        const valid = normalized.filter((c) => c.function?.name && TOOL_HANDLERS[c.function.name]);
        if (valid.length > 0) {
          toolCallCount++;
          console.log(
            `[${provider}] ðŸ›  Executing ${valid.length} tool(s) (direct fallback):`,
            valid.map((c) => c.function.name)
          );
          const toolMessages = await executeToolCalls(
            valid,
            provider,
            {
              web_search: { query: question },
              defaultQuery: question,
            },
            supabase,
            openai,
            metaCollector,
            toolTraceEmitter,
            debugMode,
            user,
            context
          );
          messages = [
            ...messages,
            {
              role: "assistant",
              content: directResp.content || null,
              ...(provider === "anthropic" ? { tool_uses: valid } : { tool_calls: valid }),
            },
            ...toolMessages,
          ];
          continue; // re-run LLM
        } else {
          console.warn(
            `[${provider}] âš ï¸ Direct fallback tool_calls present but none were valid/handled.`
          );
        }
      }

      if (directHasContent) {
        console.log(
          `[${provider}] âœ… Direct fallback returned content (${String(directResp.content).length} chars).`
        );
        yield directResp.content;
        return;
      }
    } catch (e) {
      console.warn(`[${provider}] âš ï¸ toolCalls normalization failed:`, e?.message || e);
    }
    console.warn(`[${provider}] âš ï¸ Direct fallback returned no content and no tool_calls.`);
    return;
  }

  throw new Error(`[${provider}] Limite de ${maxToolCalls} appels d'outils atteinte.`);
}

async function runHuggingFaceAgent(userQuestion, systemPrompt, modelMode) {
  const provider = "huggingface";
  const apiKey = getConfig("HUGGINGFACE_API_KEY");
  if (!apiKey) throw new Error("ClÃ© API manquante pour huggingface");

  const model =
    resolveModelForProvider(provider, modelMode) || PROVIDER_CONFIGS.huggingface.defaultModel;
  const url =
    typeof PROVIDER_CONFIGS.huggingface.apiUrl === "function"
      ? PROVIDER_CONFIGS.huggingface.apiUrl(model)
      : PROVIDER_CONFIGS.huggingface.apiUrl;

  const messages = [
    { role: "system", content: systemPrompt },
    { role: "user", content: userQuestion },
  ];

  const payload = {
    model,
    messages,
    temperature: 0.3,
    top_p: 0.95,
    stream: false,
  };

  console.log(`[huggingface] âžœ request model=${model}`);
  const resp = await fetch(url, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(payload),
  });

  console.log(`[huggingface] â¬… status=${resp.status}`);
  if (!resp.ok) {
    const body = await resp.text();
    console.error(`[huggingface] âŒ error body preview: ${previewForLog(body)}`);
    throw new Error(`huggingface API ${resp.status}: ${body}`);
  }

  const data = await resp.json();
  const text = data?.choices?.[0]?.message?.content || "";

  return String(text || "").trim();
}

export default handler;
export const config = { path: "/api/chat-stream" };
